
<html>
<body>
  <head>
    <meta charset="utf-8" >
    <title>Ehtical Reflection 1 </title>
    <meta name="description" content="This is Angie's first ethical reflection. "/>
    <meta name="robots" content="index,follow" />
  <style>
  @import url('https://fonts.googleapis.com/css?family=Roboto');
  @import url('https://fonts.googleapis.com/css?family=Amiko|Roboto');

  body {
    font-family: 'Roboto', sans-serif;
    font-size: 14;
    background-color: rgb(238, 215, 255);
    margin-left: 50px;
    margin-right: 50px;
    width: 90%;
    line-height: 1.5;
  }
#header {
  font-family: 'Amiko', sans-serif;
  text-align: center;
  font-family: ;
  width: 100%;
  margin: 0 auto;
  padding: 10px;
  background-color:rgb(193, 144, 255);
  border-radius: 5px;
  margin-top: 20px;
  border-style: hidden;
}
.heading2 {
  text-align: center;
  font-family: 'Amiko', sans-serif;
  width: 40%;
  margin: 0 auto;
  padding: 10px;
  background-color: rgb(230, 230, 230);
  border: 5px dotted white;
  border-radius: 5px;
  margin-top: 20px;

}
.heading2:hover {
  background-color:  rgb(174, 123, 198);
  color: rgb(0, 0, 0);
  transition: all 0.5s;
  color: rgb(2, 0, 4);
  background-color: rgb(214, 240, 255);
}
div.a {
  text-indent: 50px;
}
div.b {
  text-indent: 50px;

}

div.c {
    text-indent: 50px;
}
div.d {
    text-indent: 50px;
}
div.e {
    text-indent: 50px;
}
#left-wrapped-image{
  width: 300px;
 float: left;
 margin: 15px;
}
#right-wrapped-image{
  width: 140px;
  float: right;
  margin: 15px;
}
.noindent{
  text-indent: 0px;
}



</style>
</head>

  <h1 id="header"> Technology Has An Opinion</h1>
  <h2 class="heading2">  By: Angelina Collado </h2>
  <div class="a">

  <img src="https://d2w9rnfcy7mm78.cloudfront.net/1915476/large_112753bb528d8e9706b24b33fe8d4076" id="right-wrapped-image"/>
<p> <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias </a>, which described the mainly racial bias with an algorithm used to rate criminals’ likely hood of committing another crime, brought to my attention that the tools we are provided especially technological ones can be used in a specific situation that can lead to discrimination, sexism, ableism, etc. When growing up thinking about technology and the negative impacts it may have, it was usually a scenario in which a person used technology to hurt someone, or an insistence in that realm. However, after reading <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias </a>, I can’t help but be shocked because of the fact that the code and algorithm itself can lead to equally harsh situations. My idea growing up in regard to technology was that it was objective because it was a machine. Yet, the article starts off with an instance of algorithm bias that intrigues the reader, myself included; the instance was that Borden, who identified as black, was rated to be of a higher risk of committing a crime, even though her record had minor crimes, while Prater, who identified as white, was rated to be a lower risk, even if his record was longer. At the end, the article revealed that Prater was indeed arrested once again within two years and Borden wasn’t, which shows that the algorithm was incorrect.
</p>
</div>
<div class="b">
    <p> Honestly, I was slightly relieved when I learned that this didn’t determine jail sentencing but rather probation and conditions like it and that the whole decision isn’t solely based on the algorithm. I do still feel unsettled, though, because knowing the results can cloud someone’s judgment. And I was also dissatisfied with the lack of solutions included in <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias </a>. So, when I looked for other sources, I read <a href="http://www.pbs.org/wgbh/nova/next/tech/ai-bias/">Ghost in the Machine</a>, which mentioned the research from the <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">Machine Bias </a>, but also took time to describe a specific scenario that made me disappointed in our world.
   </p>
</div>
<div class="c">
      <img src="http://www.pbs.org/wgbh/nova/next/wp-content/uploads/2017/10/joy-buolamwini-coding-768x422.jpg" id="left-wrapped-image"/>
<p> When I opened the page, there is a picture of a black woman holding a white mask. This image resonated deeply with me and urged me to continue reading the article. As I was reading I learned that the woman, named Buolamwini, was an MIT student and her job was to research face analysis but the algorithms didn’t recognize her face or that she was a person. Therefore, in order to do her job, she had to wear the white mask. The article continues to explain that the reason that the algorithms didn’t recognize her face was because algorithms are usually trained by pictures of white faces, mainly males at that too, and controlled by engineers who are mainly white as well. The algorithms have been shown to be less accurate when identifying black faces than white faces as well as having inconsistencies with women and young people. And I was frustrated when I read that face analysis in Asia, recognize Asian faces better than white spaces, as if that were an excuse for American technology recognizing white faces better, especially since Asia, and <a href="https://www.japantimes.co.jp/community/2010/11/02/issues/homogeneous-unique-myths-stunt-discourse/#.W0VKE1ZKgWo">Japan</a>, specifically, is really homogenous and America isn’t.
</p>
<div class="d">
<p> While I was reading I was also wondering how can algorithms be unbiased if the coders of it are. In <a href="http://www.pbs.org/wgbh/nova/next/tech/ai-bias/">Ghost in the Machine</a>, it had mentioned that bias can be because of zip codes, racial and socieconmic bias related to those addresses, and credit scores. The article had also revealed some solutions to algorithm bias in general including testing the version of the algorithm multiple times. Buolamwini had founded a nonprofit organization group, Algorithmic Justice League, to raise awareness of bias in the world today such as in algorithms. I also learned that it may not be the smartest choice to have automated algorithms because it can be hard for humans to understand which can lead to unknown biases. There is also research being done on a way for artificial intelligence’s products to be designed to help researchers check algorithms and help see bias in the future.
</p>
</div>
<div class="e">
<p>  Reading both articles were shocking and informative. I had felt educated and aware of the extent of the problem but shocked that it isn’t being talked about more often. But that is life I guess.
    </p>
    </div>

<p class="noindent"> <em> First published July 10, 2018. </em> </p>
</body>
</html>
